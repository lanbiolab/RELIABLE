pip install munkres
pip install fastdtw
pip install scikit-learn
python main_train.py --config_file=config/Scene15.yaml
python main_train.py --config_file=config/LandUse-21.yaml
python main_train.py --config_file=config/Reuters.yaml
python main_train.py --config_file=config/Hdigit.yaml
python main_train.py --config_file=config/cub.yaml
python main_train.py --config_file=config/MNIST-USPS.yaml


python main_train.py --config_file=config/Caltech101.yaml
python main_train.py --config_file=config/NoisyMNIST.yaml






# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



def get_knn_graph(args, data):
    # print(data)
    # print(data.size())
    num_samples = data.size(0)
    graph = torch.zeros(num_samples, num_samples, dtype=torch.int32, device=data.device)

    for i in range(num_samples):
        distance = torch.sum((data - data[i]) ** 2, dim=1)
        # print(f"distance shape: {distance.shape}, args.k: {args.k}")
        # print("data 的第二个维度", data.size(1))
        k = min(args.k, data.size(1))
        # print("K的值：", k)
        # exit(1)
        _, small_indices = torch.topk(distance, k, largest=False)
        # _, small_indices = torch.topk(distance, args.k, largest=False)  # +1 to exclude self from neighbors
        # Fill 1 in the graph for the k nearest neighbors
        graph[i, small_indices[1:]] = 1

    # Ensure the graph is symmetric
    result_graph = torch.max(graph, graph.t())
    return result_graph

def get_W(mv_data, k):
    W = []
    mv_data_loader, num_views, num_samples, _ = get_all_multiview_data(mv_data)
    for iter_step, (sub_data_views, _, _) in enumerate(mv_data_loader):
        # print(iter_step)
        # print("samples的数据维度：", len(sub_data_views))
        # print(len(sub_data_views[0]))
        # print(len(sub_data_views[1]))
        for i in range(num_views):
            result_graph = get_knn_graph(sub_data_views[i], k)
            W.append(result_graph)
    # print(W)
    return W



def kernel_affinity(self, z, temperature=0.1, step: int = 5):  
        z = L2norm(z)   # 对输入的特征矩阵 z 进行 L2 归一化处理。L2norm 函数是对每个向量进行标准化，使得每个向量的 L2 范数为 1
        G = (2 - 2 * (z @ z.t())).clamp(min=0.)    # 这一行代码计算了特征矩阵 z 中样本之间的距离矩阵
        G = torch.exp(-G / temperature)     # 将负距离输入到指数函数中，得到一个高斯核（Gaussian kernel）的相似度矩阵，类似于热核（heat kernel
        G = G / G.sum(dim=1, keepdim=True) # 对相似度矩阵 G 的每一行进行归一化处理 确保所有相似度值的总和为1，这样可以将矩阵 G 视为一个条件概率矩阵，表示某个样本与其他样本之间的概率分布

        G = torch.matrix_power(G, step)    

        alpha = 0.5
        G = torch.eye(G.shape[0]).cuda() * alpha + G * (1 - alpha)  
        return G


Epoch 171 K-means: NMI = 0.3591 ARI = 0.1807 F = 0.2218 ACC = 0.3171
